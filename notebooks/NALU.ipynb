{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAC and NALU are as defined in https://arxiv.org/abs/1808.00508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.nn.init import kaiming_normal_\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NAC, self).__init__()\n",
    "        # W_hat and M_hat - the params of the NAC model\n",
    "        self.W_hat = nn.Parameter(data=torch.Tensor(output_size, input_size))\n",
    "        self.M_hat = nn.Parameter(data=torch.Tensor(output_size, input_size))\n",
    "        # NAC doesn't converge nearly as fast without this way of init!\n",
    "        kaiming_normal_(self.W_hat)\n",
    "        kaiming_normal_(self.M_hat)\n",
    "    \n",
    "    def W(self):\n",
    "        # W is computed from W_hat and M_hat params\n",
    "        return torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Affine transform of x as defined by W\n",
    "        return F.linear(input=x, weight=self.W(), bias=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NALU, self).__init__()\n",
    "        # Shared NAC\n",
    "        self.nac = NAC(input_size, output_size)\n",
    "        # Global gate params that decide between add/sub VS mul/div operation\n",
    "        self.G = nn.Linear(input_size, 1)\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def g(self, x):\n",
    "        return torch.sigmoid(self.G(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Gate (decide add/sub VS mul/div operation)\n",
    "        g = self.g(x)\n",
    "        # Result of add/sub\n",
    "        a = self.nac(x)\n",
    "        # Result of mul/div\n",
    "        log_space_x = torch.log(abs(x) + self.eps)\n",
    "        log_space_output = F.linear(input=log_space_x, weight=self.nac.W(), bias=None)\n",
    "        m = torch.exp(log_space_output)\n",
    "        # Interpolated result between a and m\n",
    "        return g * a + (1-g) * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[3., 1.]]), tensor([4.]))\n",
      "(tensor([[5., 3.]]), tensor([8.]))\n",
      "(tensor([[2., 2.]]), tensor([4.]))\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for k in range(500):\n",
    "    # Generate training \"input\" between range 1 and 6\n",
    "    a_b = torch.randint(low=1, high=6, size=(1, 2)).type(torch.FloatTensor)\n",
    "    # I. Training \"labels\" to induce an \"adder\" function\n",
    "    c =   a_b[:, 0] + a_b[:, 1]\n",
    "#     # II. Training \"labels\" to induce a \"subtractor\" function\n",
    "#     c =   a_b[:, 0] - a_b[:, 1]\n",
    "#     # III. Training \"labels\" to induce a \"multiplier\" function\n",
    "#     c =   a_b[:, 0] * a_b[:, 1]\n",
    "#     # IV. Training \"labels\" to induce an \"divider\" function\n",
    "#     c =   a_b[:, 0] / a_b[:, 1]\n",
    "    examples.append((a_b, c))\n",
    "\n",
    "for eg in examples[:3]:  print(eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init NALU\n",
    "nalu = NALU(input_size=2, output_size=1)  # Takes two inputs and returns one output\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(nalu.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00d5af661bd4c338b5c8b7c3a58ad6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "nalu.train()\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "bar_1 = tqdm(range(EPOCHS))\n",
    "for epoch in bar_1:\n",
    "    total_loss = 0\n",
    "    for a_b, c in examples:\n",
    "        c_pred = nalu(a_b).view(1)\n",
    "        \n",
    "        nalu.zero_grad()\n",
    "\n",
    "        loss = loss_function(c_pred, c)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "    bar_1.set_description(\"loss: %0.4f\" % total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic arithmetic function induced from data!\n",
    "\n",
    "def my_func(a, b):\n",
    "    \n",
    "    mode_map = {\n",
    "        0: 'div/mul',\n",
    "        1: 'add/sub'\n",
    "    }\n",
    "    \n",
    "    x = torch.Tensor([a, b])\n",
    "    nalu.eval()\n",
    "    print('NAC\\'s ~W:')\n",
    "    display(nalu.nac.W().data.numpy().round())\n",
    "    print('NALU\\'s mode:')\n",
    "    print(mode_map[int(nalu.g(x).data.numpy().round())])\n",
    "    return float(nalu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAC's ~W:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NALU's mode:\n",
      "add/sub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109.110595703125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 100\n",
    "b = 10\n",
    "\n",
    "my_func(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
